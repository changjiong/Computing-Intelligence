{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基础理论部分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0. Can you come up out 3 sceneraies which use AI methods? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: \n",
    " - 模拟人的眼耳嘴行： \n",
    " - 图像/人脸识别，fakeface，\n",
    " - 语音识别，机器翻译，\n",
    " - 智能对话，\n",
    " - 自动驾驶，\n",
    " - 机器人，\n",
    " - 购物推荐等"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. How do we use Github; Why do we use Jupyter and Pycharm;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: \n",
    "- git主要用来做版本管理和协作开发，git常用命令三步曲\n",
    "        - 建仓库 git init\n",
    "        - 添加文件和提交 git add  git commit\n",
    "        - 推到远程仓 git push\n",
    "- jupyter已经成为事实上最流行的数据科学分析工具，优势在于骨子里的交互式理念，体现在代码执行、交互演示、代码组织、教学分享各方面。\n",
    "- pycharm没怎么用过一般用jupyter和vscode，作为重型ide其优势应该在大型项目开发，比较工程向。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. What's the Probability Model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:\n",
    "> A probability model is a mathematical representation of a random phenomenon. It is defined by its sample space, events within the sample space, and probabilities associated with each event.\n",
    "http://www.stat.yale.edu/Courses/1997-98/101/probint.htm\n",
    "https://www.zweigmedia.com/RealWorld/tutorialsf15e/frames7_3.html\n",
    "\n",
    "- 简单理解，概率是大量实验中，样本空间内某事件发生的次数；概率模型是理论估计，大量实验的前提下预估某事件的分布律，从而不必真的去得出结果。\n",
    "- 与统计概率的区别：概率模型，其概率分布一般是确定的，比如说随机变量X服从正态分布，概率模型就是随机变量X及其分布律F；而对于统计模型来说，一般是不知道随机变量X的分布律的，一次试验能得到一个经验分布，多次试验就能得到多个经验分布，统计模型就是要基于这些试验结果来估计出一个分布，所以统计模型一般定义为随机变量X的分布律F的集合。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Can you came up with some sceneraies at which we could use Probability Model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:\n",
    "- 赌场，扔骰子，天气预报，外卖配送剩余时间，语音识别，输入法联想，手写体识别"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Why do we use probability and what's the difficult points for programming based on parsing and pattern match?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:\n",
    "- 概率模型的应用是因为基于规则的句法分析走到了尽头；首先是语法规则无法覆盖全部的真实语言并且写到最后还会出现规则之间互相矛盾，其次即便写出全部规则也无法用计算机全部解析，因为自然语言是上下文有关文法而计算机是上下文无关文法，在语义处理上有无法回避的分歧，最后大数据时代基于规则的算法有难以忍受的时间复杂度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. What's the Language Model;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:\n",
    "- A statistical language model is a probability distribution over sequences of words. Given such a sequence, say of length m, it assigns a probability $ P(w_{1},w_{2},\\ldots ,w_{m}) $ to the whole sequence.\n",
    "https://en.wikipedia.org/wiki/Language_model\n",
    "- 单地地说，语言模型就是用来计算一个句子的概率的模型，也就是判断一句话是否是人话的概率。\n",
    "\n",
    "- 语言模型诞生之初是贾里尼克为了解决语音识别的问题，判断一段语音合理对应一段文本序列，合理的评价标准就是其可能性大小。\n",
    "- 但是这种方法有两个问题：\n",
    "    - 参数空间太大， $P(w_{i}|w_{1},w_{2},\\ldots ,w_{i-1})$可能性太多无法估算\n",
    "    - 数据太稀疏sparsity，对于非常多词对的组合，在语料库中都没有出现，依据最大似然估计得到的概率将会是0。\n",
    "- 针对以上两个问题的解决方案\n",
    "    - Markov马尔科夫假设，即偷懒假设，随意一个词出现的概率只与它前面出现的有限的一个或者几个词有关。\n",
    "    $ P(w_{1},w_{2},\\ldots ,w_{m}) = \\prod_{i=1}^{m}P(w_{i}|w_{1},w_{2},\\ldots ,w_{i-1})\n",
    "\\approx \\prod_{i=1}^{m}P(w_{i}|w_{i-(n-1)},\\ldots ,w_{i-1}) $\n",
    "    - 古德-图灵估计，从概率总量中分配一个很小比例给未看见事件unseen events，以应对零概率问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Can you came up with some sceneraies at which we could use Language Model?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:\n",
    "- 机器翻译、语音识别、印刷体或手写体识别（光学字符识别）、拼写纠错、输入法联想"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. What's the 1-gram language model;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:\n",
    "- N-gram模型的特例，当n=1时就是1-gram模型或者Unigram模型或者词袋模型bag of words，基本假设就是语料中每个词独立出现，无条件相关。\n",
    "$ P(w_{1},w_{2},\\ldots ,w_{m}) \\approx P(w_{1}) * P(w_{2}) * \\ldots * P(w_{m}) $\n",
    "- 如何计算$P(w_{m})$概率呢？针对预料中每个词进行词频统计，根据大数定律只要统计量足够，相对频度就等于概率即\n",
    "$ P(w_{m}) \\approx \\frac{\\#w_{m}}{\\#} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. What's the disadvantages and advantages of 1-gram language model;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:\n",
    "- 优点是假设简单，操作实践都比较容易，适用于某些分类场景，在可用性达标的情况下简单高效\n",
    "- 缺点是假设过于简单，本质是上下文无关的无序模型，无法胜任合理性要求较高的场景。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. What't the 2-gram models;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:\n",
    "- N-gram模型，当n=2时就是2-gram模型或者bigram模型，基本假设就是语料中每个词出现概率和前一个词相关。\n",
    "$ P(w_{1},w_{2},\\ldots ,w_{m}) \\approx P(w_{1}|<s>) * P(w_{2}|w_{1}) * P(w_{3}|w_{2}) * \\ldots * P(w_{m}|w_{m-1}) * P(</s>|w_{m}) $\n",
    "- 模型参数计算方法$$ P(w_i|w_{i-1}) \\approx \\frac{\\#(w_{i-1}, w_i)}{\\#(w_{i-1})} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 编程实践部分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 设计你自己的句子生成器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如何生成句子是一个很经典的问题，从1940s开始，图灵提出机器智能的时候，就使用的是人类能不能流畅和计算机进行对话。和计算机对话的一个前提是，计算机能够生成语言。\n",
    "\n",
    "计算机如何能生成语言是一个经典但是又很复杂的问题。 我们课程上为大家介绍的是一种基于规则（Rule Based）的生成方法。该方法虽然提出的时间早，但是现在依然在很多地方能够大显身手。值得说明的是，现在很多很实用的算法，都是很久之前提出的，例如，二分查找提出与1940s, Dijstra算法提出于1960s 等等。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这一部分，需要各位同学首先定义自己的语言。 大家可以先想一个应用场景，然后在这个场景下，定义语法。例如：\n",
    "\n",
    "在西部世界里，一个”人类“的语言可以定义为：\n",
    "``` \n",
    "human = \"\"\"\n",
    "human = 自己 寻找 活动\n",
    "自己 = 我 | 俺 | 我们 \n",
    "寻找 = 看看 | 找找 | 想找点\n",
    "活动 = 乐子 | 玩的\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "一个“接待员”的语言可以定义为\n",
    "```\n",
    "host = \"\"\"\n",
    "host = 寒暄 报数 询问 业务相关 结尾 \n",
    "报数 = 我是 数字 号 ,\n",
    "数字 = 单个数字 | 数字 单个数字 \n",
    "单个数字 = 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 \n",
    "寒暄 = 称谓 打招呼 | 打招呼\n",
    "称谓 = 人称 ,\n",
    "人称 = 先生 | 女士 | 小朋友\n",
    "打招呼 = 你好 | 您好 \n",
    "询问 = 请问你要 | 您需要\n",
    "业务相关 = 玩玩 具体业务\n",
    "玩玩 = 耍一耍 | 玩一玩\n",
    "具体业务 = 喝酒 | 打牌 | 打猎 | 赌博\n",
    "结尾 = 吗？\"\"\"\n",
    "\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**定义你自己的语法:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "garbage_classification = \"\"\"\n",
    "垃圾分类 -> 具体垃圾 谓语 分类垃圾\n",
    "具体垃圾 -> 饮料盒 | 报纸 | 药瓶 | 易拉罐 | 毛巾 |电视机 | 蓄电池 |\\\n",
    "        节能灯 |温度计 | 剩面条 | 果皮 | 奶酪 | 猫粮 |厕纸 | 陶瓷 | \\\n",
    "        大骨头 |鱼虾 | 调味料 | 中药渣 | 肉蛋 | 菌菇 | 花卉 | 豆腐\n",
    "谓语 -> 副词 动词\n",
    "副词 -> 很显然 | 有可能 | 不可能\n",
    "动词 -> 属于\n",
    "分类垃圾 -> 有害垃圾 | 可回收垃圾 | 湿垃圾 | 干垃圾\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from computing_intelligence.utils_lesson1 import generate_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_n(garbage_classification, '垃圾分类')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 使用新数据源完成语言模型的训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "按照我们上文中定义的`prob_2`函数，我们更换一个文本数据源，获得新的Language Model:\n",
    "\n",
    "1. 下载文本数据集（你可以在以下数据集中任选一个，也可以两个都使用）\n",
    "    + 可选数据集1，保险行业问询对话集： https://github.com/Computing-Intelligence/insuranceqa-corpus-zh/raw/release/corpus/pool/train.txt.gz\n",
    "    + 可选数据集2：豆瓣评论数据集：https://github.com/Computing-Intelligence/datasource/raw/master/movie_comments.csv\n",
    "2. 修改代码，获得新的**2-gram**语言模型\n",
    "    + 进行文本清洗，获得所有的纯文本\n",
    "    + 将这些文本进行切词\n",
    "    + 送入之前定义的语言模型中，判断文本的合理程度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___N-gram模型，基本假设就是语料中每个词出现概率和前n-1个词相关。___\n",
    "$$ P(w_{1},w_{2},\\ldots ,w_{m}) = \\prod_{i=1}^{m}P(w_{i}|w_{1},w_{2},\\ldots ,w_{i-1})\n",
    "\\approx \\prod_{i=1}^{m}P(w_{i}|w_{i-(n-1)},\\ldots ,w_{i-1}) $$\n",
    "- 模型参数计算方法$$ P(w_{i}|w_{i-(n-1)},\\ldots ,w_{i-1}) \\approx \\frac{\\#(w_{i-(n-1)},\\ldots ,w_{i})}{\\#(w_{i-(n-1)},\\ldots ,w_{i-1})} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\quad $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from computing_intelligence.utils_lesson1 import(get_clean_text,\n",
    "                                                get_token,\n",
    "                                                get_sentence_probablity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_comments = '..\\..\\data\\movie_comments.csv'\n",
    "insurance_corpus = '..\\..\\data\\insuranceqa-corpus-train.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "#### 3. 获得最优质的的语言"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当我们能够生成随机的语言并且能判断之后，我们就可以生成更加合理的语言了。请定义 generate_best 函数，该函数输入一个语法 + 语言模型，能够生成**n**个句子，并能选择一个最合理的句子:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "提示，要实现这个函数，你需要Python的sorted函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from computing_intelligence.utils_lesson1 import generate_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_best(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "好了，现在我们实现了自己的第一个AI模型，这个模型能够生成比较接近于人类的语言。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: 这个模型有什么问题？ 你准备如何提升？ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: 稀疏数据造成的零概率问题；语料选取问题，语料与模型应用差异太大，效果大打折扣；训练数据不足。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. (Optional) 完成基于Pattern Match的语句问答\n",
    "> 我们的GitHub仓库中，有一个assignment-01-optional-pattern-match，这个难度较大，感兴趣的同学可以挑战一下。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 5. (Optional) 完成阿兰图灵机器智能原始论文的阅读\n",
    "1. 请阅读阿兰图灵关于机器智能的原始论文：https://github.com/Computing-Intelligence/References/blob/master/AI%20%26%20Machine%20Learning/Computer%20Machinery%20and%20Intelligence.pdf \n",
    "2. 并按照GitHub仓库中的论文阅读模板，填写完毕后发送给我: mqgao@kaikeba.com 谢谢"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "各位同学，我们已经完成了自己的第一个AI模型，大家对人工智能可能已经有了一些感觉，人工智能的核心就是，我们如何设计一个模型、程序，在外部的输入变化的时候，我们的程序不变，依然能够解决问题。人工智能是一个很大的领域，目前大家所熟知的深度学习只是其中一小部分，之后也肯定会有更多的方法提出来，但是大家知道人工智能的目标，就知道了之后进步的方向。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后，希望大家对AI不要有恐惧感，这个并不难，大家加油！"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
